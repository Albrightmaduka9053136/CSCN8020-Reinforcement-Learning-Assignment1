{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fccb41",
   "metadata": {},
   "source": [
    "**Name:** Albright Maduka\n",
    "\n",
    "**Student ID:** 9053136  \n",
    "\n",
    "**Course:** CSCN 8020  (Reinforcement Learning Programming)\n",
    "\n",
    "**Solution to Assignment 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257cce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "os.makedirs(\"logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6e990",
   "metadata": {},
   "source": [
    "## Problem 1: Pick-and-Place Robot as an MDP\n",
    "\n",
    "We model the pick-and-place robot control problem as a Markov Decision Process (MDP) defined by the tuple \\( (S, A, P, R, \\gamma) \\).\n",
    "\n",
    "**State space (S):**  \n",
    "The state describes the robot and task configuration, including joint positions and velocities, the end-effector position, the gripper state (open/closed), and the object position.\n",
    "\n",
    "**Action space (A):**  \n",
    "Actions are low-level motor commands, such as joint torque or joint velocity commands, that move the robot.\n",
    "\n",
    "**Transition model (P):**  \n",
    "Given the current state and an action, the next state is determined by the robot’s dynamics. If noise is present, transitions can be stochastic, but they depend only on the current state and action, satisfying the Markov property.\n",
    "\n",
    "**Reward function (R):**  \n",
    "The agent receives a large positive reward for successfully placing the object at the target location. A small negative reward is given at each time step to encourage faster completion. Additional penalties are applied for collisions, dropping the object, or unsafe/jerky movements. This encourages fast, smooth, and safe motions.\n",
    "\n",
    "**Discount factor (\\(gamma\\)):**  \n",
    "A discount factor \\( gamma in (0,1) \\) is used to balance short-term efficiency and long-term task success.\n",
    "\n",
    "**Justification:**  \n",
    "This MDP formulation allows the agent to learn motor-level control policies that trade off speed, smoothness, and task success using only the current robot and task state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930e584",
   "metadata": {},
   "source": [
    "## Problem 2: 2×2 Gridworld\n",
    "\n",
    "I have four states arranged as:\n",
    "\n",
    "s1  s2  \n",
    "s3  s4  \n",
    "\n",
    "Rewards:  \n",
    "R(s1) = 5, R(s2) = 10, R(s3) = 1, R(s4) = 2  \n",
    "\n",
    "Discount factor: \\( gamma = 0.95 \\)\n",
    "\n",
    "Transitions are deterministic. Invalid moves keep the agent in the same state.\n",
    "\n",
    "Initial policy: for all states, \\( pi_0(\\text{up}\\mid s)=1 \\).\n",
    "\n",
    "\n",
    "\n",
    "### A) Two Iterations (Sweeps) of Value Iteration\n",
    "\n",
    "Bellman optimality update:\n",
    "\\[\n",
    "V_{k+1}(s) = R(s) + \\gamma \\max_a V_k(s')\n",
    "\\]\n",
    "\n",
    "**Initialization:**\n",
    "\\[\n",
    "V_0(s)=0 \\quad \\text{for all } s\n",
    "\\]\n",
    "\n",
    "#### Iteration / Sweep 1\n",
    "Since \\(V_0(s')=0\\) for all next states:\n",
    "\\[\n",
    "V_1(s)=R(s)\n",
    "\\]\n",
    "\n",
    "So:\n",
    "- \\(V_1(s_1)=5\\)\n",
    "- \\(V_1(s_2)=10\\)\n",
    "- \\(V_1(s_3)=1\\)\n",
    "- \\(V_1(s_4)=2\\)\n",
    "\n",
    "#### Iteration / Sweep 2 (showing the max calculation explicitly)\n",
    "\n",
    "**From \\(s_1\\)** (right→\\(s_2\\), down→\\(s_3\\), up/left invalid→stay \\(s_1\\)):\n",
    "- up: \\(s'=s_1\\) ⇒ \\(V_1(s')=V_1(s_1)=5\\)\n",
    "- left: \\(s'=s_1\\) ⇒ \\(V_1(s')=V_1(s_1)=5\\)\n",
    "- right: \\(s'=s_2\\) ⇒ \\(V_1(s')=V_1(s_2)=10\\)\n",
    "- down: \\(s'=s_3\\) ⇒ \\(V_1(s')=V_1(s_3)=1\\)\n",
    "\n",
    "\\[\n",
    "\\max_a V_1(s')=\\max(5,5,10,1)=10\n",
    "\\]\n",
    "\\[\n",
    "V_2(s_1)=R(s_1)+0.95\\cdot 10=5+0.95\\cdot 10=14.5\n",
    "\\]\n",
    "\n",
    "**From \\(s_2\\)** (left→\\(s_1\\), down→\\(s_4\\), up/right invalid→stay \\(s_2\\)):\n",
    "- up: \\(s'=s_2\\) ⇒ \\(V_1(s')=V_1(s_2)=10\\)\n",
    "- right: \\(s'=s_2\\) ⇒ \\(V_1(s')=V_1(s_2)=10\\)\n",
    "- left: \\(s'=s_1\\) ⇒ \\(V_1(s')=V_1(s_1)=5\\)\n",
    "- down: \\(s'=s_4\\) ⇒ \\(V_1(s')=V_1(s_4)=2\\)\n",
    "\n",
    "\\[\n",
    "\\max_a V_1(s')=\\max(10,10,5,2)=10\n",
    "\\]\n",
    "\\[\n",
    "V_2(s_2)=R(s_2)+0.95\\cdot 10=10+0.95\\cdot 10=19.5\n",
    "\\]\n",
    "\n",
    "**From \\(s_3\\)** (up→\\(s_1\\), right→\\(s_4\\), left/down invalid→stay \\(s_3\\)):\n",
    "- up: \\(s'=s_1\\) ⇒ \\(V_1(s')=V_1(s_1)=5\\)\n",
    "- left: \\(s'=s_3\\) ⇒ \\(V_1(s')=V_1(s_3)=1\\)\n",
    "- right: \\(s'=s_4\\) ⇒ \\(V_1(s')=V_1(s_4)=2\\)\n",
    "- down: \\(s'=s_3\\) ⇒ \\(V_1(s')=V_1(s_3)=1\\)\n",
    "\n",
    "\\[\n",
    "\\max_a V_1(s')=\\max(5,1,2,1)=5\n",
    "\\]\n",
    "\\[\n",
    "V_2(s_3)=R(s_3)+0.95\\cdot 5=1+0.95\\cdot 5=5.75\n",
    "\\]\n",
    "\n",
    "**From \\(s_4\\)** (up→\\(s_2\\), left→\\(s_3\\), right/down invalid→stay \\(s_4\\)):\n",
    "- up: \\(s'=s_2\\) ⇒ \\(V_1(s')=V_1(s_2)=10\\)\n",
    "- right: \\(s'=s_4\\) ⇒ \\(V_1(s')=V_1(s_4)=2\\)\n",
    "- left: \\(s'=s_3\\) ⇒ \\(V_1(s')=V_1(s_3)=1\\)\n",
    "- down: \\(s'=s_4\\) ⇒ \\(V_1(s')=V_1(s_4)=2\\)\n",
    "\n",
    "\\[\n",
    "\\max_a V_1(s')=\\max(10,2,1,2)=10\n",
    "\\]\n",
    "\\[\n",
    "V_2(s_4)=R(s_4)+0.95\\cdot 10=2+0.95\\cdot 10=11.5\n",
    "\\]\n",
    "\n",
    "So after two sweeps:\n",
    "- \\(V_2(s_1)=14.5\\)\n",
    "- \\(V_2(s_2)=19.5\\)\n",
    "- \\(V_2(s_3)=5.75\\)\n",
    "- \\(V_2(s_4)=11.5\\)\n",
    "\n",
    "\n",
    "\n",
    "### B) Policy Evaluation and Policy Improvement (explicitly, starting from \\( \\pi_0 \\))\n",
    "\n",
    "Initial policy \\( \\pi_0 \\): always choose **Up** in every state.\n",
    "\n",
    "#### Round 1 — Policy Evaluation (solve exactly)\n",
    "\n",
    "Under \\( \\pi_0 \\):\n",
    "- \\(s_1\\) --U--> \\(s_1\\) (invalid up, stays)\n",
    "- \\(s_2\\) --U--> \\(s_2\\) (invalid up, stays)\n",
    "- \\(s_3\\) --U--> \\(s_1\\)\n",
    "- \\(s_4\\) --U--> \\(s_2\\)\n",
    "\n",
    "Bellman expectation equations:\n",
    "\\[\n",
    "V(s_1)=5+0.95V(s_1)\\Rightarrow V(s_1)=\\frac{5}{1-0.95}=100\n",
    "\\]\n",
    "\\[\n",
    "V(s_2)=10+0.95V(s_2)\\Rightarrow V(s_2)=\\frac{10}{1-0.95}=200\n",
    "\\]\n",
    "\\[\n",
    "V(s_3)=1+0.95V(s_1)=1+0.95(100)=96\n",
    "\\]\n",
    "\\[\n",
    "V(s_4)=2+0.95V(s_2)=2+0.95(200)=192\n",
    "\\]\n",
    "\n",
    "So:\n",
    "- \\(V(s_1)=100\\), \\(V(s_2)=200\\), \\(V(s_3)=96\\), \\(V(s_4)=192\\)\n",
    "\n",
    "#### Round 1 — Policy Improvement (greedy w.r.t. \\(V\\))\n",
    "\n",
    "Choose the action that leads to the next state with the highest value:\n",
    "\n",
    "- At \\(s_1\\): best is right to \\(s_2\\) (200) ⇒ \\( \\pi_1(s_1)=R \\)\n",
    "- At \\(s_2\\): staying in \\(s_2\\) via up/right gives 200 (best) ⇒ \\( \\pi_1(s_2)=U \\) (or \\(R\\))\n",
    "- At \\(s_3\\): best is up to \\(s_1\\) (100) ⇒ \\( \\pi_1(s_3)=U \\)\n",
    "- At \\(s_4\\): best is up to \\(s_2\\) (200) ⇒ \\( \\pi_1(s_4)=U \\)\n",
    "\n",
    "Thus:\n",
    "\\[\n",
    "\\pi_1=\\{s_1:R,\\; s_2:U,\\; s_3:U,\\; s_4:U\\}\n",
    "\\]\n",
    "\n",
    "#### Round 2 — Policy Evaluation (solve exactly)\n",
    "\n",
    "Under \\( \\pi_1 \\):\n",
    "- \\(s_2\\) --U--> \\(s_2\\):\n",
    "\\[\n",
    "V(s_2)=10+0.95V(s_2)\\Rightarrow V(s_2)=200\n",
    "\\]\n",
    "- \\(s_1\\) --R--> \\(s_2\\):\n",
    "\\[\n",
    "V(s_1)=5+0.95V(s_2)=5+0.95(200)=195\n",
    "\\]\n",
    "- \\(s_4\\) --U--> \\(s_2\\):\n",
    "\\[\n",
    "V(s_4)=2+0.95V(s_2)=2+0.95(200)=192\n",
    "\\]\n",
    "- \\(s_3\\) --U--> \\(s_1\\):\n",
    "\\[\n",
    "V(s_3)=1+0.95V(s_1)=1+0.95(195)=186.25\n",
    "\\]\n",
    "\n",
    "#### Round 2 — Policy Improvement\n",
    "\n",
    "Checking greedy actions again, the policy does not change (it is stable).  \n",
    "\n",
    "**Final improved policy:**\n",
    "\\[\n",
    "\\pi^*=\\{s_1:R,\\; s_2:U,\\; s_3:U,\\; s_4:U\\}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c768e",
   "metadata": {},
   "source": [
    "## Problem 3: 5×5 Gridworld with Value Iteration\n",
    "\n",
    "In this problem, we consider a 5×5 gridworld Markov Decision Process (MDP) with deterministic transitions. States are identified by their (row, column) indices. The goal (terminal) state is at (4,4) with reward +10. Three grey (non-favourable) states, (2,2), (3,0), and (0,4), have reward −5, while all other states have reward −1. If an action would move the agent outside the grid, the agent remains in the same state.\n",
    "\n",
    "The discount factor is set to \\( gamma = 0.95 \\). We first define the reward function based on whether a state is terminal, grey, or a regular state. Then, we apply Value Iteration to compute the optimal state-value function \\( V^* \\) and the corresponding optimal policy \\( pi^* \\). The stopping criterion is based on a small tolerance \\( theta \\), and convergence is detected when the maximum change in values across all states becomes smaller than \\( \\theta \\).\n",
    "\n",
    "In addition to the standard (synchronous) Value Iteration, we also implement an in-place version, where updated values are immediately reused within the same sweep. Both methods are run until convergence, and the resulting \\( V^* \\) and \\( pi^* \\) are printed as 5×5 tables.\n",
    "\n",
    "Finally, we compare the two methods in terms of the number of iterations and computation time. In this experiment, both synchronous and in-place Value Iteration converge to the same optimal value function and policy, and they require the same number of iterations due to the small size of the grid. The computational complexity per sweep is \\( O(|S||A|) \\), where \\( |S| \\) is the number of states and \\( |A| \\) is the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d2f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 complete. Log written to: logs/problem3.log\n",
      "\n",
      "V* (γ=0.95):\n",
      " [[-0.097387  0.950119  2.052757  3.213428  0.435187]\n",
      " [ 0.950119  2.052757  3.213428  4.435187  5.72125 ]\n",
      " [ 2.052757  3.213428  0.435187  5.72125   7.075   ]\n",
      " [-0.786572  4.435187  5.72125   7.075     8.5     ]\n",
      " [ 4.435187  5.72125   7.075     8.5      10.      ]]\n",
      "\n",
      "π*:\n",
      " [['R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'T']]\n",
      "\n",
      "Comparison:\n",
      "  Max |V_sync - V_inplace| = 0.000e+00\n",
      "  Policies identical?      = True\n",
      "  Sync iterations/time     = 10 0.005190s\n",
      "  InPlace iterations/time  = 10 0.002002s\n"
     ]
    }
   ],
   "source": [
    "# 5x5 Gridworld\n",
    "# ENVIRONMENT SPECIFICATION\n",
    "\n",
    "# Grid size 5x5 (states identified by (row, col))\n",
    "N = 5\n",
    "\n",
    "# Terminal/goal state (episode ends when reached)\n",
    "GOAL = (4, 4)\n",
    "\n",
    "# Grey (non-favourable) states (valid states but reward = -5)\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "# Action set: Right, Down, Left, Up\n",
    "# We represent each action as a (dr, dc) movement on the grid.\n",
    "ACTIONS = {\n",
    "    \"R\": (0, 1),\n",
    "    \"D\": (1, 0),\n",
    "    \"L\": (0, -1),\n",
    "    \"U\": (-1, 0),\n",
    "}\n",
    "\n",
    "def in_bounds(r: int, c: int) -> bool:\n",
    "    \"\"\"Return True if (r,c) is inside the grid.\"\"\"\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"\n",
    "    Reward function R(s) as described in the assignment:\n",
    "    +10 if s is GOAL\n",
    "    -5  if s is in GREY\n",
    "    -1  otherwise\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return 10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"\n",
    "    Deterministic transition:\n",
    "    - If action is valid -> move to next state\n",
    "    - If action hits a wall -> stay in same state\n",
    "    - If already at terminal -> remain in terminal (absorbing)\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return s  # absorbing terminal\n",
    "    dr, dc = ACTIONS[a]\n",
    "    r, c = s\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        return s  # invalid move -> stay\n",
    "    return (nr, nc)\n",
    "\n",
    "def all_states():\n",
    "    \"\"\"List of all states in the grid.\"\"\"\n",
    "    return [(r, c) for r in range(N) for c in range(N)]\n",
    "\n",
    "# UTILITY: FORMATTING TABLES\n",
    "\n",
    "\n",
    "def V_to_grid(V):\n",
    "    \"\"\"\n",
    "    Convert V dict {(r,c): value} into a numpy array for printing/logging.\n",
    "    \"\"\"\n",
    "    grid = np.zeros((N, N), dtype=float)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            grid[r, c] = V[(r, c)]\n",
    "    return grid\n",
    "\n",
    "def greedy_policy_from_V(V):\n",
    "    \"\"\"\n",
    "    Derive greedy policy pi(s) = argmax_a V(s') using deterministic transitions.\n",
    "    If tie: Python's max() picks first by internal order; that's fine unless your rubric demands tie-handling.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for s in all_states():\n",
    "        if s == GOAL:\n",
    "            pi[s] = \"T\"  # terminal\n",
    "        else:\n",
    "            pi[s] = max(ACTIONS.keys(), key=lambda a: V[step(s, a)])\n",
    "    return pi\n",
    "\n",
    "def policy_to_grid(pi):\n",
    "    \"\"\"Convert policy dict into 5x5 grid of action symbols for printing/logging.\"\"\"\n",
    "    grid = np.empty((N, N), dtype=object)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            grid[r, c] = pi[(r, c)]\n",
    "    return grid\n",
    "\n",
    "\n",
    "# VALUE ITERATION (SYNCHRONOUS)\n",
    "\n",
    "def value_iteration_sync(gamma=0.95, theta=1e-8, max_iters=100000, logger=None):\n",
    "    \"\"\"\n",
    "    Synchronous (two-array) Value Iteration:\n",
    "    - compute V_new from V (old) for all states\n",
    "    - then replace V <- V_new at the end of sweep\n",
    "\n",
    "    Stopping rule:\n",
    "    - stop if delta = max_s |V_new(s) - V(s)| < theta\n",
    "    \"\"\"\n",
    "    S = all_states()\n",
    "    V = {s: 0.0 for s in S}  # initialize values to 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "\n",
    "        for s in S:\n",
    "            if s == GOAL:\n",
    "                # Terminal state value: set it to +10 (consistent with reward and terminal semantics)\n",
    "                V_new[s] = 10.0\n",
    "                continue\n",
    "\n",
    "            # Bellman optimality backup:\n",
    "            # V(s) = R(s) + gamma * max_a V(s')\n",
    "            best_next = max(V[step(s, a)] for a in ACTIONS)\n",
    "            V_new[s] = reward(s) + gamma * best_next\n",
    "\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "        # Log every iteration (you can change to log every k iterations if you want smaller logs)\n",
    "        if logger:\n",
    "            logger.info(f\"[VI-SYNC] iter={it:05d} delta={delta:.12f}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            elapsed = time.time() - t0\n",
    "            return V, it, elapsed\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    return V, max_iters, elapsed\n",
    "\n",
    "\n",
    "# VALUE ITERATION (IN-PLACE)\n",
    "\n",
    "\n",
    "def value_iteration_inplace(gamma=0.95, theta=1e-8, max_iters=100000, logger=None):\n",
    "    \"\"\"\n",
    "    In-place Value Iteration:\n",
    "    - update V(s) immediately and use updated values in the same sweep\n",
    "\n",
    "    Often converges in fewer sweeps due to faster propagation of new information.\n",
    "    \"\"\"\n",
    "    S = all_states()\n",
    "    V = {s: 0.0 for s in S}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in S:\n",
    "            old = V[s]\n",
    "\n",
    "            if s == GOAL:\n",
    "                V[s] = 10.0\n",
    "            else:\n",
    "                best_next = max(V[step(s, a)] for a in ACTIONS)\n",
    "                V[s] = reward(s) + gamma * best_next\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old))\n",
    "\n",
    "        if logger:\n",
    "            logger.info(f\"[VI-INPLACE] iter={it:05d} delta={delta:.12f}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            elapsed = time.time() - t0\n",
    "            return V, it, elapsed\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    return V, max_iters, elapsed\n",
    "\n",
    "\n",
    "#  MAIN: RUN + LOG + REPORT\n",
    "\n",
    "def main():\n",
    "    # Logging to logs/problem3.log\n",
    "    logging.basicConfig(\n",
    "        filename=\"logs/problem3.log\",\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(message)s\",\n",
    "        force=True  # IMPORTANT in notebooks\n",
    "    )\n",
    "    logger = logging.getLogger(\"Problem3\")\n",
    "\n",
    "    # Parameters (explicit for marking)\n",
    "    gamma = 0.95\n",
    "    theta = 1e-8\n",
    "    max_iters = 100000\n",
    "\n",
    "    logger.info(\"==========  Problem 3 ==========\")\n",
    "    logger.info(f\"Grid size: {N}x{N}\")\n",
    "    logger.info(f\"Goal state: {GOAL} (terminal)\")\n",
    "    logger.info(f\"Grey states: {sorted(GREY)}\")\n",
    "    logger.info(\"Rewards: +10 at GOAL, -5 at GREY, -1 otherwise\")\n",
    "    logger.info(f\"Parameters: gamma={gamma}, theta={theta}, max_iters={max_iters}\")\n",
    "    logger.info(\"Actions: R, D, L, U (deterministic); invalid move => stay\")\n",
    "    logger.info(\"------------------------------------------------------\")\n",
    "\n",
    "    # 1) Synchronous VI\n",
    "    V_sync, it_sync, t_sync = value_iteration_sync(\n",
    "        gamma=gamma, theta=theta, max_iters=max_iters, logger=logger\n",
    "    )\n",
    "    pi_sync = greedy_policy_from_V(V_sync)\n",
    "\n",
    "    logger.info(\"----- Final Results: Synchronous Value Iteration -----\")\n",
    "    logger.info(f\"Converged in {it_sync} iterations, time={t_sync:.6f} sec\")\n",
    "    logger.info(\"V* (rounded to 6):\\n\" + str(np.round(V_to_grid(V_sync), 6)))\n",
    "    logger.info(\"pi*:\\n\" + str(policy_to_grid(pi_sync)))\n",
    "\n",
    "    # 2) In-place VI\n",
    "    V_inp, it_inp, t_inp = value_iteration_inplace(\n",
    "        gamma=gamma, theta=theta, max_iters=max_iters, logger=logger\n",
    "    )\n",
    "    pi_inp = greedy_policy_from_V(V_inp)\n",
    "\n",
    "    logger.info(\"----- Final Results: In-Place Value Iteration -----\")\n",
    "    logger.info(f\"Converged in {it_inp} iterations, time={t_inp:.6f} sec\")\n",
    "    logger.info(\"V* (rounded to 6):\\n\" + str(np.round(V_to_grid(V_inp), 6)))\n",
    "    logger.info(\"pi*:\\n\" + str(policy_to_grid(pi_inp)))\n",
    "\n",
    "    # 3) Comparison\n",
    "    max_abs_diff = max(abs(V_sync[s] - V_inp[s]) for s in V_sync)\n",
    "    same_policy = all(pi_sync[s] == pi_inp[s] for s in pi_sync)\n",
    "\n",
    "    logger.info(\"----- Comparison (Sync vs In-Place) -----\")\n",
    "    logger.info(f\"Max abs difference in V: {max_abs_diff:.12e}\")\n",
    "    logger.info(f\"Policies identical? {same_policy}\")\n",
    "    logger.info(f\"Sync: iterations={it_sync}, time={t_sync:.6f}\")\n",
    "    logger.info(f\"InPlace: iterations={it_inp}, time={t_inp:.6f}\")\n",
    "    logger.info(\"======================================================\")\n",
    "\n",
    "    # Console output\n",
    "    print(\"Problem 3 complete. Log written to: logs/problem3.log\")\n",
    "    print(\"\\nV* (γ=0.95):\\n\", np.round(V_to_grid(V_sync), 6))\n",
    "    print(\"\\nπ*:\\n\", policy_to_grid(pi_sync))\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"  Max |V_sync - V_inplace| =\", f\"{max_abs_diff:.3e}\")\n",
    "    print(\"  Policies identical?      =\", same_policy)\n",
    "    print(\"  Sync iterations/time     =\", it_sync, f\"{t_sync:.6f}s\")\n",
    "    print(\"  InPlace iterations/time  =\", it_inp, f\"{t_inp:.6f}s\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b30a7",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "The optimal policy mainly selects **Right (R)** and **Down (D)** actions, which move the agent efficiently toward the goal at (4,4). The goal state has value 10, which is consistent with the terminal reward, while the grey states have noticeably lower values (including a negative value near (3,0)) due to the −5 penalty, making those regions less desirable.\n",
    "\n",
    "Both the synchronous and in-place implementations of Value Iteration converged to the same optimal value function and policy under the tolerance \\( \\theta = 10^{-8} \\). In this experiment, both methods required the same number of iterations, which can occur in small deterministic problems. Since each sweep evaluates all states and actions, the computational cost per sweep is \\( O(|S||A|) \\), meaning it grows proportionally with the number of states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fecc2",
   "metadata": {},
   "source": [
    "## Problem 4: Off-policy Monte Carlo with Importance Sampling\n",
    "\n",
    "This section:\n",
    "1. Defines the same 5×5 Gridworld as in Problem 3.\n",
    "2. Implements off-policy Monte Carlo with Weighted Importance Sampling.\n",
    "3. Estimates the state-value function V(s) from the learned action-value function Q(s,a).\n",
    "4. Compares the Monte Carlo estimate with the optimal value function V* obtained from Value Iteration.\n",
    "5. Logs the results to `problem4.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4 complete. Log written to: logs/problem4.log\n",
      "\n",
      "V_MC (γ=0.95):\n",
      " [[-0.102  0.938  2.025  3.172  0.403]\n",
      " [ 0.896  2.012  3.181  4.401  5.693]\n",
      " [ 1.857  3.155  0.414  5.696  7.05 ]\n",
      " [-0.816  4.394  5.69   7.052  8.486]\n",
      " [ 4.166  5.673  7.046  8.49  10.   ]]\n",
      "\n",
      "π_MC:\n",
      " [['D' 'R' 'D' 'D' 'D']\n",
      " ['D' 'R' 'R' 'D' 'D']\n",
      " ['R' 'D' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'T']]\n",
      "\n",
      "Comparison vs VI (same γ):\n",
      "  VI time = 0.000000s\n",
      "  MAE(|V_MC - V*|) = 0.045186\n",
      "  Max(|V_MC - V*|) = 0.269031\n"
     ]
    }
   ],
   "source": [
    "# Off-policy Monte Carlo with Importance Sampling (Weighted IS)\n",
    "# ENVIRONMENT SPECIFICATION\n",
    "\n",
    "# Grid size: 5x5\n",
    "N = 5\n",
    "\n",
    "# Terminal/goal state (episode ends when reached)\n",
    "GOAL = (4, 4)\n",
    "\n",
    "# Grey (non-favourable) states with penalty -5\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "# Action set: Right, Down, Left, Up\n",
    "# Each action is represented as (dr, dc) movement on the grid\n",
    "ACTIONS = {\n",
    "    \"R\": (0, 1),\n",
    "    \"D\": (1, 0),\n",
    "    \"L\": (0, -1),\n",
    "    \"U\": (-1, 0),\n",
    "}\n",
    "\n",
    "def in_bounds(r, c):\n",
    "    \"\"\"Check if (r,c) is inside the grid.\"\"\"\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"\n",
    "    Reward function R(s) (state-based, as defined in the assignment):\n",
    "    +10 for goal state,\n",
    "    -5 for grey states,\n",
    "    -1 for all other states.\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return 10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"\n",
    "    Deterministic transition function.\n",
    "    If action would leave the grid, the agent stays in the same state.\n",
    "    If state is terminal, it remains there.\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return s\n",
    "    dr, dc = ACTIONS[a]\n",
    "    r, c = s\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        return s\n",
    "    return (nr, nc)\n",
    "\n",
    "def all_states():\n",
    "    \"\"\"Return a list of all states in the grid.\"\"\"\n",
    "    return [(r, c) for r in range(N) for c in range(N)]\n",
    "\n",
    "def V_to_grid(V):\n",
    "    \"\"\"Convert dictionary V(s) to a 5x5 numpy grid for printing.\"\"\"\n",
    "    grid = np.zeros((N, N), dtype=float)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            grid[r, c] = V[(r, c)]\n",
    "    return grid\n",
    "\n",
    "def policy_to_grid(pi):\n",
    "    \"\"\"Convert policy dictionary to a 5x5 grid of actions.\"\"\"\n",
    "    return np.array([[pi[(r, c)] for c in range(N)] for r in range(N)], dtype=object)\n",
    "\n",
    "# VALUE ITERATION (FOR COMPARISON AGAINST MC)\n",
    "\n",
    "def value_iteration_sync(gamma=0.95, theta=1e-8, max_iters=100000):\n",
    "    \"\"\"\n",
    "    Standard synchronous Value Iteration to compute the optimal V*.\n",
    "    Used for comparison against the Monte Carlo estimate.\n",
    "\n",
    "    gamma: discount factor\n",
    "    theta: convergence tolerance (stop when max change < theta)\n",
    "    \"\"\"\n",
    "    S = all_states()\n",
    "    V = {s: 0.0 for s in S}\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "\n",
    "        for s in S:\n",
    "            if s == GOAL:\n",
    "                V_new[s] = 10.0\n",
    "                continue\n",
    "\n",
    "            best_next = max(V[step(s, a)] for a in ACTIONS)\n",
    "            V_new[s] = reward(s) + gamma * best_next\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "# BEHAVIOR POLICY b(a|s)\n",
    "\n",
    "def behavior_policy(_s):\n",
    "    \"\"\"\n",
    "    Fixed behavior policy b(a|s): uniform random over {R, D, L, U}.\n",
    "    \"\"\"\n",
    "    return random.choice(list(ACTIONS.keys()))\n",
    "\n",
    "def behavior_prob(_s, _a):\n",
    "    \"\"\"Probability under behavior policy (uniform over 4 actions).\"\"\"\n",
    "    return 1.0 / len(ACTIONS)\n",
    "\n",
    "# OFF-POLICY MC (WEIGHTED IMPORTANCE SAMPLING)\n",
    "\n",
    "def off_policy_mc_weighted_is(\n",
    "    gamma=0.95,\n",
    "    num_episodes=50000,\n",
    "    max_steps=200,\n",
    "    seed=0,\n",
    "    log_every=1000,\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Off-policy Monte Carlo control with Weighted Importance Sampling (WIS).\n",
    "\n",
    "    Q(s,a): action-value function\n",
    "    C(s,a): cumulative IS weights\n",
    "\n",
    "    Target policy pi: greedy w.r.t Q (deterministic).\n",
    "    Behavior policy b: uniform random.\n",
    "\n",
    "    Backward update:\n",
    "      G = gamma*G + r\n",
    "      C(s,a) += W\n",
    "      Q(s,a) += (W/C(s,a)) * (G - Q(s,a))\n",
    "\n",
    "    Weight update (when action matches greedy):\n",
    "      W *= (pi/b) = 1 / 0.25 = 4\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    S = all_states()\n",
    "    Q = {(s, a): 0.0 for s in S for a in ACTIONS}\n",
    "    C = {(s, a): 0.0 for s in S for a in ACTIONS}\n",
    "\n",
    "    def greedy_action(s):\n",
    "        \"\"\"Greedy action under current Q.\"\"\"\n",
    "        return max(ACTIONS.keys(), key=lambda a: Q[(s, a)])\n",
    "\n",
    "    t0 = time.time()\n",
    "    recent_lengths = []\n",
    "    recent_maxW = 0.0\n",
    "\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "\n",
    "        # Generate one episode using behavior policy b\n",
    "        s = (0, 0)   # start state\n",
    "        episode = [] # list of (s, a, r)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            a = behavior_policy(s)\n",
    "\n",
    "            # NOTE: rewards are state-based R(s) in this assignment\n",
    "            r = reward(s)\n",
    "            episode.append((s, a, r))\n",
    "\n",
    "            if s == GOAL:\n",
    "                break\n",
    "            s = step(s, a)\n",
    "\n",
    "        recent_lengths.append(len(episode))\n",
    "\n",
    "        # Backward return updates\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "\n",
    "        for (s, a, r) in reversed(episode):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "\n",
    "            # If not greedy action under target policy, pi(a|s)=0 -> stop\n",
    "            if a != greedy_action(s):\n",
    "                break\n",
    "\n",
    "            # W = W * (pi/b). Here pi=1 and b=1/4\n",
    "            W *= (1.0 / behavior_prob(s, a))\n",
    "            recent_maxW = max(recent_maxW, W)\n",
    "\n",
    "        # Logging progress\n",
    "        if logger and ep % log_every == 0:\n",
    "            avg_len = float(np.mean(recent_lengths)) if recent_lengths else 0.0\n",
    "            logger.info(\n",
    "                f\"[MC-WIS] ep={ep:06d} avg_len={avg_len:.2f} recent_maxW={recent_maxW:.2f}\"\n",
    "            )\n",
    "            recent_lengths = []\n",
    "            recent_maxW = 0.0\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Derive V(s) and greedy policy pi(s) from Q\n",
    "    V = {s: max(Q[(s, a)] for a in ACTIONS) for s in S}\n",
    "    pi = {s: (\"T\" if s == GOAL else greedy_action(s)) for s in S}\n",
    "\n",
    "    return V, pi, elapsed\n",
    "\n",
    "# MAIN: RUN + LOG + REPORT\n",
    "\n",
    "def main():\n",
    "    # Logging to logs/problem4.log\n",
    "    logging.basicConfig(\n",
    "        filename=\"logs/problem4.log\",\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(message)s\",\n",
    "        force=True  # IMPORTANT in notebooks\n",
    "    )\n",
    "    logger = logging.getLogger(\"Problem4\")\n",
    "\n",
    "    gamma = 0.95\n",
    "    episodes = 50000\n",
    "    max_steps = 200\n",
    "    seed = 0\n",
    "    log_every = 1000\n",
    "\n",
    "    logger.info(\"========== Problem 4 ==========\")\n",
    "    logger.info(f\"Grid size: {N}x{N}\")\n",
    "    logger.info(f\"Goal state: {GOAL} (terminal)\")\n",
    "    logger.info(f\"Grey states: {sorted(GREY)}\")\n",
    "    logger.info(\"Rewards: +10 at GOAL, -5 at GREY, -1 otherwise\")\n",
    "    logger.info(\"Behavior policy b(a|s): uniform random over {R,D,L,U} -> b=0.25 each\")\n",
    "    logger.info(\"Target policy pi(a|s): greedy wrt Q (deterministic)\")\n",
    "    logger.info(\"Importance Sampling: Weighted IS (lower variance than ordinary IS)\")\n",
    "    logger.info(f\"Parameters: gamma={gamma}, episodes={episodes}, max_steps={max_steps}, seed={seed}\")\n",
    "    logger.info(\"------------------------------------------------------\")\n",
    "\n",
    "    # Run Monte Carlo (WIS)\n",
    "    V_mc, pi_mc, t_mc = off_policy_mc_weighted_is(\n",
    "        gamma=gamma,\n",
    "        num_episodes=episodes,\n",
    "        max_steps=max_steps,\n",
    "        seed=seed,\n",
    "        log_every=log_every,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    logger.info(\"----- Final Results (MC Estimate) -----\")\n",
    "    logger.info(f\"Time={t_mc:.6f} sec\")\n",
    "    logger.info(\"V_MC (rounded to 3):\\n\" + str(np.round(V_to_grid(V_mc), 3)))\n",
    "    logger.info(\"pi_MC:\\n\" + str(policy_to_grid(pi_mc)))\n",
    "\n",
    "    # Compare MC estimate to Value Iteration optimum\n",
    "    t0_vi = time.time()\n",
    "    V_vi = value_iteration_sync(gamma=gamma, theta=1e-8)\n",
    "    t_vi = time.time() - t0_vi\n",
    "\n",
    "    grid_mc = V_to_grid(V_mc)\n",
    "    grid_vi = V_to_grid(V_vi)\n",
    "\n",
    "    mae = float(np.mean(np.abs(grid_mc - grid_vi)))\n",
    "    max_err = float(np.max(np.abs(grid_mc - grid_vi)))\n",
    "\n",
    "    logger.info(\"----- Comparison: MC vs Value Iteration -----\")\n",
    "    logger.info(f\"VI time={t_vi:.6f} sec\")\n",
    "    logger.info(f\"MAE(|V_MC - V*|) = {mae:.6f}\")\n",
    "    logger.info(f\"Max(|V_MC - V*|) = {max_err:.6f}\")\n",
    "    logger.info(\"======================================================\")\n",
    "\n",
    "    # Console output\n",
    "    print(\"Problem 4 complete. Log written to: logs/problem4.log\")\n",
    "    print(\"\\nV_MC (γ=0.95):\\n\", np.round(grid_mc, 3))\n",
    "    print(\"\\nπ_MC:\\n\", policy_to_grid(pi_mc))\n",
    "    print(\"\\nComparison vs VI (same γ):\")\n",
    "    print(\"  VI time =\", f\"{t_vi:.6f}s\")\n",
    "    print(\"  MAE(|V_MC - V*|) =\", f\"{mae:.6f}\")\n",
    "    print(\"  Max(|V_MC - V*|) =\", f\"{max_err:.6f}\")\n",
    "\n",
    "main() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d36094",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "Using off-policy Monte Carlo with Weighted Importance Sampling and a discount factor \\( gamma = 0.95 \\), the estimated value function \\( V_{\\{MC}} \\) and greedy policy \\( pi_{\\{MC}} \\) were obtained, with the goal state value close to 10 and the grey states showing lower values due to the −5 penalty. The learned policy mainly selects **Right (R)** and **Down (D)** actions, similar to the optimal policy from Value Iteration. Comparing \\( V_{\\{MC}} \\) with the optimal value function \\( V^* \\) gives a mean absolute error of about 0.045 and a maximum error of about 0.269, indicating that Monte Carlo provides a close but approximate solution due to sampling noise and a finite number of episodes. In terms of performance, Monte Carlo methods require many episodes and their cost grows with episode length, whereas Value Iteration is a model-based method with per-iteration complexity \\( O(|S||A|) \\) that converges in a small number of sweeps for this problem, highlighting the trade-off between model-free sampling methods and dynamic programming approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
