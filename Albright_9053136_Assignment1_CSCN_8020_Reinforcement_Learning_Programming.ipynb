{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fccb41",
   "metadata": {},
   "source": [
    "**Name:** Albright Maduka\n",
    "\n",
    "**Student ID:** 9053136  \n",
    "\n",
    "**Course:** CSCN 8020  (Reinforcement Learning Programming)\n",
    "\n",
    "**Solution to Assignment 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257cce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "os.makedirs(\"logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6e990",
   "metadata": {},
   "source": [
    "## Problem 1: Pick-and-Place Robot as an MDP\n",
    "\n",
    "We model the task as a Markov Decision Process (MDP):\n",
    "(MDP) = (S, A, P, R, γ), where γ = 0.95 is the discount factor.\n",
    "\n",
    "### State (S)\n",
    "To satisfy the Markov property and allow smooth control, the state includes:\n",
    "s_t = (θ_t, θ̇_t, x^ee_t, ẋ^ee_t, g_t, x^obj_t, x^target_t)\n",
    "\n",
    "Where:\n",
    "- θ_t: joint angles\n",
    "- θ̇_t: joint velocities (needed for smooth motion)\n",
    "- x^ee_t, ẋ^ee_t: end-effector pose and velocity\n",
    "- g_t: gripper state (holding object or not)\n",
    "- x^obj_t: object pose\n",
    "- x^target_t: target pose\n",
    "\n",
    "This state is Markov because it contains all information needed to predict future states.\n",
    "\n",
    "### Action  (A)\n",
    "The agent controls the robot motors directly. A natural choice is:\n",
    "a_t = τ_t ∈ R^n\n",
    "where τ_t are joint torques (one per joint).\n",
    "Alternatives include motor voltages or desired joint velocities.\n",
    "\n",
    "### Transition Model P(s' | s, a)\n",
    "The transition is given by robot dynamics:\n",
    "s_{t+1} = f(s_t, a_t) + ε\n",
    "where ε represents noise, friction, and modeling errors.\n",
    "Collisions, joint limits, or contacts may clamp the next state or terminate the episode.\n",
    "\n",
    "### Reward Function R(s, a, s')\n",
    "We want fast, smooth, safe, and successful execution:\n",
    "R_t =\n",
    "+100 if the object is placed correctly\n",
    "+20  if a successful grasp is achieved\n",
    "-λ1 ||τ_t||^2            (penalize large torques)\n",
    "-λ2 ||τ_t - τ_{t-1}||^2  (penalize jerky motions)\n",
    "-λ3 Δt                   (penalize time to encourage speed)\n",
    "-200 if collision or object drop occurs\n",
    "\n",
    "This reward encourages:\n",
    "- Correct task completion\n",
    "- Smooth and energy-efficient motion\n",
    "- Safety (avoid collisions)\n",
    "- Speed (finish quickly)\n",
    "\n",
    "### Termination\n",
    "An episode terminates when:\n",
    "- The object is placed successfully, or\n",
    "- A collision, or\n",
    "- A time limit is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930e584",
   "metadata": {},
   "source": [
    "## Problem 2: 2×2 Gridworld\n",
    "\n",
    "I have four states arranged as:\n",
    "\n",
    "s1  s2\n",
    "s3  s4\n",
    "\n",
    "Rewards:\n",
    "R(s1) = 5, R(s2) = 10, R(s3) = 1, R(s4) = 2\n",
    "Discount factor: γ = 0.95\n",
    "\n",
    "Transitions are deterministic. Invalid moves keep the agent in the same state.\n",
    "\n",
    "Although Problem 2 refers to Value Iteration, it also mentions policy evaluation and policy improvement. Therefore, both approaches are presented here for completeness.\n",
    "\n",
    "---\n",
    "\n",
    "### A) Two Sweeps of Value Iteration\n",
    "\n",
    "Bellman optimality update:\n",
    "V_{k+1}(s) = R(s) + γ * max_a V_k(s')\n",
    "\n",
    "Initialize:\n",
    "V0(s) = 0 for all s\n",
    "\n",
    "#### Sweep 1\n",
    "Since V0(s') = 0:\n",
    "V1(s) = R(s)\n",
    "\n",
    "So:\n",
    "V1(s1) = 5\n",
    "V1(s2) = 10\n",
    "V1(s3) = 1\n",
    "V1(s4) = 2\n",
    "\n",
    "#### Sweep 2\n",
    "Compute best next-state values:\n",
    "\n",
    "- From s1: best next is s2 with value 10\n",
    "  V2(s1) = 5 + 0.95 * 10 = 14.5\n",
    "\n",
    "- From s2: best next is staying in s2 (invalid moves keep you there)\n",
    "  V2(s2) = 10 + 0.95 * 10 = 19.5\n",
    "\n",
    "- From s3: best next is s1 with value 5\n",
    "  V2(s3) = 1 + 0.95 * 5 = 5.75\n",
    "\n",
    "- From s4: best next is s2 with value 10\n",
    "  V2(s4) = 2 + 0.95 * 10 = 11.5\n",
    "\n",
    "So after two sweeps:\n",
    "V2(s1) = 14.5\n",
    "V2(s2) = 19.5\n",
    "V2(s3) = 5.75\n",
    "V2(s4) = 11.5\n",
    "\n",
    "---\n",
    "\n",
    "### B) Policy Evaluation + Policy Improvement\n",
    "\n",
    "Initial policy π0: always move UP in every state.\n",
    "\n",
    "#### Round 1 — Policy Evaluation\n",
    "\n",
    "Transitions under π0:\n",
    "s1 → s1, s2 → s2, s3 → s1, s4 → s2\n",
    "\n",
    "Solve:\n",
    "V(s1) = 5 + 0.95 V(s1)  → V(s1) = 100\n",
    "V(s2) = 10 + 0.95 V(s2) → V(s2) = 200\n",
    "V(s3) = 1 + 0.95 V(s1)  → V(s3) = 96\n",
    "V(s4) = 2 + 0.95 V(s2)  → V(s4) = 192\n",
    "\n",
    "#### Round 1 — Policy Improvement\n",
    "\n",
    "Choose best action in each state:\n",
    "s1 → Right (to s2)\n",
    "s2 → Up (stay)\n",
    "s3 → Right (to s4)\n",
    "s4 → Up (to s2)\n",
    "\n",
    "So:\n",
    "π1 = { s1:R, s2:U, s3:R, s4:U }\n",
    "\n",
    "#### Round 2 — Policy Evaluation\n",
    "\n",
    "V(s2) = 200\n",
    "V(s1) = 5 + 0.95 * 200 = 195\n",
    "V(s4) = 2 + 0.95 * 200 = 192\n",
    "V(s3) = 1 + 0.95 * 192 = 183.4\n",
    "\n",
    "#### Round 2 — Policy Improvement\n",
    "\n",
    "At s3:\n",
    "Up → s1 gives 195\n",
    "Right → s4 gives 192\n",
    "So choose Up.\n",
    "\n",
    "Final improved policy:\n",
    "π2 = { s1:R, s2:U, s3:U, s4:U }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c768e",
   "metadata": {},
   "source": [
    "### Problem 3: 5×5 Gridworld with Value Iteration\n",
    "\n",
    "In this problem, we consider a 5×5 gridworld Markov Decision Process (MDP) with deterministic transitions. States are identified by their (row, column) indices. The goal (terminal) state is at (4,4) with reward +10. Three grey (non-favourable) states, (2,2), (3,0), and (0,4), have reward −5, while all other states have reward −1. If an action would move the agent outside the grid, the agent remains in the same state.\n",
    "\n",
    "The discount factor is set to \\( gamma = 0.95 \\). We first define the reward function based on whether a state is terminal, grey, or a regular state. Then, we apply Value Iteration to compute the optimal state-value function \\( V^* \\) and the corresponding optimal policy \\( pi^* \\). The stopping criterion is based on a small tolerance \\( theta \\), and convergence is detected when the maximum change in values across all states becomes smaller than \\( \\theta \\).\n",
    "\n",
    "In addition to the standard (synchronous) Value Iteration, we also implement an in-place version, where updated values are immediately reused within the same sweep. Both methods are run until convergence, and the resulting \\( V^* \\) and \\( pi^* \\) are printed as 5×5 tables.\n",
    "\n",
    "Finally, we compare the two methods in terms of the number of iterations and computation time. In this experiment, both synchronous and in-place Value Iteration converge to the same optimal value function and policy, and they require the same number of iterations due to the small size of the grid. The computational complexity per sweep is \\( O(|S||A|) \\), where \\( |S| \\) is the number of states and \\( |A| \\) is the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d2f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 complete. Log written to: logs/problem3.log\n",
      "\n",
      "V* (γ=0.95):\n",
      " [[-0.097387  0.950119  2.052757  3.213428  0.435187]\n",
      " [ 0.950119  2.052757  3.213428  4.435187  5.72125 ]\n",
      " [ 2.052757  3.213428  0.435187  5.72125   7.075   ]\n",
      " [-0.786572  4.435187  5.72125   7.075     8.5     ]\n",
      " [ 4.435187  5.72125   7.075     8.5      10.      ]]\n",
      "\n",
      "π*:\n",
      " [['R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'T']]\n",
      "\n",
      "Comparison:\n",
      "  Max |V_sync - V_inplace| = 0.000e+00\n",
      "  Policies identical?      = True\n",
      "  Sync iterations/time     = 10 0.005190s\n",
      "  InPlace iterations/time  = 10 0.002002s\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: 5x5 Gridworld\n",
    "# ENVIRONMENT SPECIFICATION\n",
    "\n",
    "# Grid size 5x5 (states identified by (row, col))\n",
    "N = 5\n",
    "\n",
    "# Terminal/goal state (episode ends when reached)\n",
    "GOAL = (4, 4)\n",
    "\n",
    "# Grey (non-favourable) states (valid states but reward = -5)\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "# Action set: Right, Down, Left, Up\n",
    "# We represent each action as a (dr, dc) movement on the grid.\n",
    "ACTIONS = {\n",
    "    \"R\": (0, 1),\n",
    "    \"D\": (1, 0),\n",
    "    \"L\": (0, -1),\n",
    "    \"U\": (-1, 0),\n",
    "}\n",
    "\n",
    "def in_bounds(r: int, c: int) -> bool:\n",
    "    \"\"\"Return True if (r,c) is inside the grid.\"\"\"\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"\n",
    "    Reward function R(s) as described in the assignment:\n",
    "    +10 if s is GOAL\n",
    "    -5  if s is in GREY\n",
    "    -1  otherwise\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return 10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"\n",
    "    Deterministic transition:\n",
    "    - If action is valid -> move to next state\n",
    "    - If action hits a wall -> stay in same state\n",
    "    - If already at terminal -> remain in terminal (absorbing)\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return s  # absorbing terminal\n",
    "    dr, dc = ACTIONS[a]\n",
    "    r, c = s\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        return s  # invalid move -> stay\n",
    "    return (nr, nc)\n",
    "\n",
    "def all_states():\n",
    "    \"\"\"List of all states in the grid.\"\"\"\n",
    "    return [(r, c) for r in range(N) for c in range(N)]\n",
    "\n",
    "# UTILITY: FORMATTING TABLES\n",
    "\n",
    "\n",
    "def V_to_grid(V):\n",
    "    \"\"\"\n",
    "    Convert V dict {(r,c): value} into a numpy array for printing/logging.\n",
    "    \"\"\"\n",
    "    grid = np.zeros((N, N), dtype=float)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            grid[r, c] = V[(r, c)]\n",
    "    return grid\n",
    "\n",
    "def greedy_policy_from_V(V):\n",
    "    \"\"\"\n",
    "    Derive greedy policy pi(s) = argmax_a V(s') using deterministic transitions.\n",
    "    If tie: Python's max() picks first by internal order; that's fine unless your rubric demands tie-handling.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for s in all_states():\n",
    "        if s == GOAL:\n",
    "            pi[s] = \"T\"  # terminal\n",
    "        else:\n",
    "            pi[s] = max(ACTIONS.keys(), key=lambda a: V[step(s, a)])\n",
    "    return pi\n",
    "\n",
    "def policy_to_grid(pi):\n",
    "    \"\"\"Convert policy dict into 5x5 grid of action symbols for printing/logging.\"\"\"\n",
    "    grid = np.empty((N, N), dtype=object)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            grid[r, c] = pi[(r, c)]\n",
    "    return grid\n",
    "\n",
    "\n",
    "# VALUE ITERATION (SYNCHRONOUS)\n",
    "\n",
    "def value_iteration_sync(gamma=0.95, theta=1e-8, max_iters=100000, logger=None):\n",
    "    \"\"\"\n",
    "    Synchronous (two-array) Value Iteration:\n",
    "    - compute V_new from V (old) for all states\n",
    "    - then replace V <- V_new at the end of sweep\n",
    "\n",
    "    Stopping rule:\n",
    "    - stop if delta = max_s |V_new(s) - V(s)| < theta\n",
    "    \"\"\"\n",
    "    S = all_states()\n",
    "    V = {s: 0.0 for s in S}  # initialize values to 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "\n",
    "        for s in S:\n",
    "            if s == GOAL:\n",
    "                # Terminal state value: set it to +10 (consistent with reward and terminal semantics)\n",
    "                V_new[s] = 10.0\n",
    "                continue\n",
    "\n",
    "            # Bellman optimality backup:\n",
    "            # V(s) = R(s) + gamma * max_a V(s')\n",
    "            best_next = max(V[step(s, a)] for a in ACTIONS)\n",
    "            V_new[s] = reward(s) + gamma * best_next\n",
    "\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "        # Log every iteration (you can change to log every k iterations if you want smaller logs)\n",
    "        if logger:\n",
    "            logger.info(f\"[VI-SYNC] iter={it:05d} delta={delta:.12f}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            elapsed = time.time() - t0\n",
    "            return V, it, elapsed\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    return V, max_iters, elapsed\n",
    "\n",
    "\n",
    "# VALUE ITERATION (IN-PLACE)\n",
    "\n",
    "\n",
    "def value_iteration_inplace(gamma=0.95, theta=1e-8, max_iters=100000, logger=None):\n",
    "    \"\"\"\n",
    "    In-place Value Iteration:\n",
    "    - update V(s) immediately and use updated values in the same sweep\n",
    "\n",
    "    Often converges in fewer sweeps due to faster propagation of new information.\n",
    "    \"\"\"\n",
    "    S = all_states()\n",
    "    V = {s: 0.0 for s in S}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in S:\n",
    "            old = V[s]\n",
    "\n",
    "            if s == GOAL:\n",
    "                V[s] = 10.0\n",
    "            else:\n",
    "                best_next = max(V[step(s, a)] for a in ACTIONS)\n",
    "                V[s] = reward(s) + gamma * best_next\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old))\n",
    "\n",
    "        if logger:\n",
    "            logger.info(f\"[VI-INPLACE] iter={it:05d} delta={delta:.12f}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            elapsed = time.time() - t0\n",
    "            return V, it, elapsed\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    return V, max_iters, elapsed\n",
    "\n",
    "\n",
    "#  MAIN: RUN + LOG + REPORT\n",
    "\n",
    "def main():\n",
    "    # Logging to logs/problem3.log\n",
    "    logging.basicConfig(\n",
    "        filename=\"logs/problem3.log\",\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(message)s\",\n",
    "        force=True  # IMPORTANT in notebooks\n",
    "    )\n",
    "    logger = logging.getLogger(\"Problem3\")\n",
    "\n",
    "    # Parameters (explicit for marking)\n",
    "    gamma = 0.95\n",
    "    theta = 1e-8\n",
    "    max_iters = 100000\n",
    "\n",
    "    logger.info(\"==========  Problem 3 ==========\")\n",
    "    logger.info(f\"Grid size: {N}x{N}\")\n",
    "    logger.info(f\"Goal state: {GOAL} (terminal)\")\n",
    "    logger.info(f\"Grey states: {sorted(GREY)}\")\n",
    "    logger.info(\"Rewards: +10 at GOAL, -5 at GREY, -1 otherwise\")\n",
    "    logger.info(f\"Parameters: gamma={gamma}, theta={theta}, max_iters={max_iters}\")\n",
    "    logger.info(\"Actions: R, D, L, U (deterministic); invalid move => stay\")\n",
    "    logger.info(\"------------------------------------------------------\")\n",
    "\n",
    "    # 1) Synchronous VI\n",
    "    V_sync, it_sync, t_sync = value_iteration_sync(\n",
    "        gamma=gamma, theta=theta, max_iters=max_iters, logger=logger\n",
    "    )\n",
    "    pi_sync = greedy_policy_from_V(V_sync)\n",
    "\n",
    "    logger.info(\"----- Final Results: Synchronous Value Iteration -----\")\n",
    "    logger.info(f\"Converged in {it_sync} iterations, time={t_sync:.6f} sec\")\n",
    "    logger.info(\"V* (rounded to 6):\\n\" + str(np.round(V_to_grid(V_sync), 6)))\n",
    "    logger.info(\"pi*:\\n\" + str(policy_to_grid(pi_sync)))\n",
    "\n",
    "    # 2) In-place VI\n",
    "    V_inp, it_inp, t_inp = value_iteration_inplace(\n",
    "        gamma=gamma, theta=theta, max_iters=max_iters, logger=logger\n",
    "    )\n",
    "    pi_inp = greedy_policy_from_V(V_inp)\n",
    "\n",
    "    logger.info(\"----- Final Results: In-Place Value Iteration -----\")\n",
    "    logger.info(f\"Converged in {it_inp} iterations, time={t_inp:.6f} sec\")\n",
    "    logger.info(\"V* (rounded to 6):\\n\" + str(np.round(V_to_grid(V_inp), 6)))\n",
    "    logger.info(\"pi*:\\n\" + str(policy_to_grid(pi_inp)))\n",
    "\n",
    "    # 3) Comparison\n",
    "    max_abs_diff = max(abs(V_sync[s] - V_inp[s]) for s in V_sync)\n",
    "    same_policy = all(pi_sync[s] == pi_inp[s] for s in pi_sync)\n",
    "\n",
    "    logger.info(\"----- Comparison (Sync vs In-Place) -----\")\n",
    "    logger.info(f\"Max abs difference in V: {max_abs_diff:.12e}\")\n",
    "    logger.info(f\"Policies identical? {same_policy}\")\n",
    "    logger.info(f\"Sync: iterations={it_sync}, time={t_sync:.6f}\")\n",
    "    logger.info(f\"InPlace: iterations={it_inp}, time={t_inp:.6f}\")\n",
    "    logger.info(\"======================================================\")\n",
    "\n",
    "    # Console output\n",
    "    print(\"Problem 3 complete. Log written to: logs/problem3.log\")\n",
    "    print(\"\\nV* (γ=0.95):\\n\", np.round(V_to_grid(V_sync), 6))\n",
    "    print(\"\\nπ*:\\n\", policy_to_grid(pi_sync))\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"  Max |V_sync - V_inplace| =\", f\"{max_abs_diff:.3e}\")\n",
    "    print(\"  Policies identical?      =\", same_policy)\n",
    "    print(\"  Sync iterations/time     =\", it_sync, f\"{t_sync:.6f}s\")\n",
    "    print(\"  InPlace iterations/time  =\", it_inp, f\"{t_inp:.6f}s\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b30a7",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "The optimal policy mainly selects **Right (R)** and **Down (D)** actions, which move the agent efficiently toward the goal at (4,4). The goal state has value 10, which is consistent with the terminal reward, while the grey states have noticeably lower values (including a negative value near (3,0)) due to the −5 penalty, making those regions less desirable.\n",
    "\n",
    "Both the synchronous and in-place implementations of Value Iteration converged to the same optimal value function and policy under the tolerance \\( \\theta = 10^{-8} \\). In this experiment, both methods required the same number of iterations, which can occur in small deterministic problems. Since each sweep evaluates all states and actions, the computational cost per sweep is \\( O(|S||A|) \\), meaning it grows proportionally with the number of states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fecc2",
   "metadata": {},
   "source": [
    "### Problem 4: Off-policy Monte Carlo with Importance Sampling\n",
    "\n",
    "This section:\n",
    "1. Defines the same 5×5 Gridworld as in Problem 3.\n",
    "2. Implements off-policy Monte Carlo with Weighted Importance Sampling.\n",
    "3. Estimates the state-value function V(s) from the learned action-value function Q(s,a).\n",
    "4. Compares the Monte Carlo estimate with the optimal value function V* obtained from Value Iteration.\n",
    "5. Logs the results to `problem4.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4 complete. Log written to: logs/problem4.log\n",
      "\n",
      "V_MC (γ=0.95):\n",
      " [[-0.102  0.938  2.025  3.172  0.403]\n",
      " [ 0.896  2.012  3.181  4.401  5.693]\n",
      " [ 1.857  3.155  0.414  5.696  7.05 ]\n",
      " [-0.816  4.394  5.69   7.052  8.486]\n",
      " [ 4.166  5.673  7.046  8.49  10.   ]]\n",
      "\n",
      "π_MC:\n",
      " [['D' 'R' 'D' 'D' 'D']\n",
      " ['D' 'R' 'R' 'D' 'D']\n",
      " ['R' 'D' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'T']]\n",
      "\n",
      "Comparison vs VI (same γ):\n",
      "  VI time = 0.000000s\n",
      "  MAE(|V_MC - V*|) = 0.045186\n",
      "  Max(|V_MC - V*|) = 0.269031\n"
     ]
    }
   ],
   "source": [
    "# Problem 4: Off-policy Monte Carlo with Importance Sampling (Weighted IS)\n",
    "# ENVIRONMENT SPECIFICATION\n",
    "\n",
    "# Grid size: 5x5\n",
    "N = 5\n",
    "\n",
    "# Terminal/goal state (episode ends when reached)\n",
    "GOAL = (4, 4)\n",
    "\n",
    "# Grey (non-favourable) states with penalty -5\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "# Action set: Right, Down, Left, Up\n",
    "# Each action is represented as (dr, dc) movement on the grid\n",
    "ACTIONS = {\n",
    "    \"R\": (0, 1),\n",
    "    \"D\": (1, 0),\n",
    "    \"L\": (0, -1),\n",
    "    \"U\": (-1, 0),\n",
    "}\n",
    "\n",
    "def in_bounds(r, c):\n",
    "    \"\"\"Check if (r,c) is inside the grid.\"\"\"\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"\n",
    "    Reward function R(s) (state-based, as defined in the assignment):\n",
    "    +10 for goal state,\n",
    "    -5 for grey states,\n",
    "    -1 for all other states.\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return 10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"\n",
    "    Deterministic transition function.\n",
    "    If action would leave the grid, the agent stays in the same state.\n",
    "    If state is terminal, it remains there.\n",
    "    \"\"\"\n",
    "    if s == GOAL:\n",
    "        return s\n",
    "    dr, dc = ACTIONS[a]\n",
    "    r, c = s\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        return s\n",
    "    return (nr, nc)\n",
    "\n",
    "def all_states():\n",
    "    \"\"\"Return a list of all states in the grid.\"\"\"\n",
    "    return [(r, c) for r in range(N) for c in range(N)]\n",
    "\n",
    "def V_to_grid(V):\n",
    "    \"\"\"Convert dictionary V(s) to a 5x5 numpy grid for printing.\"\"\"\n",
    "    grid = np.zeros((N, N), dtype=float)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            grid[r, c] = V[(r, c)]\n",
    "    return grid\n",
    "\n",
    "def policy_to_grid(pi):\n",
    "    \"\"\"Convert policy dictionary to a 5x5 grid of actions.\"\"\"\n",
    "    return np.array([[pi[(r, c)] for c in range(N)] for r in range(N)], dtype=object)\n",
    "\n",
    "# VALUE ITERATION (FOR COMPARISON AGAINST MC)\n",
    "\n",
    "def value_iteration_sync(gamma=0.95, theta=1e-8, max_iters=100000):\n",
    "    \"\"\"\n",
    "    Standard synchronous Value Iteration to compute the optimal V*.\n",
    "    Used for comparison against the Monte Carlo estimate.\n",
    "\n",
    "    gamma: discount factor\n",
    "    theta: convergence tolerance (stop when max change < theta)\n",
    "    \"\"\"\n",
    "    S = all_states()\n",
    "    V = {s: 0.0 for s in S}\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "\n",
    "        for s in S:\n",
    "            if s == GOAL:\n",
    "                V_new[s] = 10.0\n",
    "                continue\n",
    "\n",
    "            best_next = max(V[step(s, a)] for a in ACTIONS)\n",
    "            V_new[s] = reward(s) + gamma * best_next\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "# BEHAVIOR POLICY b(a|s)\n",
    "\n",
    "def behavior_policy(_s):\n",
    "    \"\"\"\n",
    "    Fixed behavior policy b(a|s): uniform random over {R, D, L, U}.\n",
    "    \"\"\"\n",
    "    return random.choice(list(ACTIONS.keys()))\n",
    "\n",
    "def behavior_prob(_s, _a):\n",
    "    \"\"\"Probability under behavior policy (uniform over 4 actions).\"\"\"\n",
    "    return 1.0 / len(ACTIONS)\n",
    "\n",
    "# OFF-POLICY MC (WEIGHTED IMPORTANCE SAMPLING)\n",
    "\n",
    "def off_policy_mc_weighted_is(\n",
    "    gamma=0.95,\n",
    "    num_episodes=50000,\n",
    "    max_steps=200,\n",
    "    seed=0,\n",
    "    log_every=1000,\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Off-policy Monte Carlo control with Weighted Importance Sampling (WIS).\n",
    "\n",
    "    Q(s,a): action-value function\n",
    "    C(s,a): cumulative IS weights\n",
    "\n",
    "    Target policy pi: greedy w.r.t Q (deterministic).\n",
    "    Behavior policy b: uniform random.\n",
    "\n",
    "    Backward update:\n",
    "      G = gamma*G + r\n",
    "      C(s,a) += W\n",
    "      Q(s,a) += (W/C(s,a)) * (G - Q(s,a))\n",
    "\n",
    "    Weight update (when action matches greedy):\n",
    "      W *= (pi/b) = 1 / 0.25 = 4\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    S = all_states()\n",
    "    Q = {(s, a): 0.0 for s in S for a in ACTIONS}\n",
    "    C = {(s, a): 0.0 for s in S for a in ACTIONS}\n",
    "\n",
    "    def greedy_action(s):\n",
    "        \"\"\"Greedy action under current Q.\"\"\"\n",
    "        return max(ACTIONS.keys(), key=lambda a: Q[(s, a)])\n",
    "\n",
    "    t0 = time.time()\n",
    "    recent_lengths = []\n",
    "    recent_maxW = 0.0\n",
    "\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "\n",
    "        # Generate one episode using behavior policy b\n",
    "        s = (0, 0)   # start state\n",
    "        episode = [] # list of (s, a, r)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            a = behavior_policy(s)\n",
    "\n",
    "            # NOTE: rewards are state-based R(s) in this assignment\n",
    "            r = reward(s)\n",
    "            episode.append((s, a, r))\n",
    "\n",
    "            if s == GOAL:\n",
    "                break\n",
    "            s = step(s, a)\n",
    "\n",
    "        recent_lengths.append(len(episode))\n",
    "\n",
    "        # Backward return updates\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "\n",
    "        for (s, a, r) in reversed(episode):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "\n",
    "            # If not greedy action under target policy, pi(a|s)=0 -> stop\n",
    "            if a != greedy_action(s):\n",
    "                break\n",
    "\n",
    "            # W = W * (pi/b). Here pi=1 and b=1/4\n",
    "            W *= (1.0 / behavior_prob(s, a))\n",
    "            recent_maxW = max(recent_maxW, W)\n",
    "\n",
    "        # Logging progress\n",
    "        if logger and ep % log_every == 0:\n",
    "            avg_len = float(np.mean(recent_lengths)) if recent_lengths else 0.0\n",
    "            logger.info(\n",
    "                f\"[MC-WIS] ep={ep:06d} avg_len={avg_len:.2f} recent_maxW={recent_maxW:.2f}\"\n",
    "            )\n",
    "            recent_lengths = []\n",
    "            recent_maxW = 0.0\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Derive V(s) and greedy policy pi(s) from Q\n",
    "    V = {s: max(Q[(s, a)] for a in ACTIONS) for s in S}\n",
    "    pi = {s: (\"T\" if s == GOAL else greedy_action(s)) for s in S}\n",
    "\n",
    "    return V, pi, elapsed\n",
    "\n",
    "# MAIN: RUN + LOG + REPORT\n",
    "\n",
    "def main():\n",
    "    # Logging to logs/problem4.log\n",
    "    logging.basicConfig(\n",
    "        filename=\"logs/problem4.log\",\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(message)s\",\n",
    "        force=True  # IMPORTANT in notebooks\n",
    "    )\n",
    "    logger = logging.getLogger(\"Problem4\")\n",
    "\n",
    "    gamma = 0.95\n",
    "    episodes = 50000\n",
    "    max_steps = 200\n",
    "    seed = 0\n",
    "    log_every = 1000\n",
    "\n",
    "    logger.info(\"========== Problem 4 ==========\")\n",
    "    logger.info(f\"Grid size: {N}x{N}\")\n",
    "    logger.info(f\"Goal state: {GOAL} (terminal)\")\n",
    "    logger.info(f\"Grey states: {sorted(GREY)}\")\n",
    "    logger.info(\"Rewards: +10 at GOAL, -5 at GREY, -1 otherwise\")\n",
    "    logger.info(\"Behavior policy b(a|s): uniform random over {R,D,L,U} -> b=0.25 each\")\n",
    "    logger.info(\"Target policy pi(a|s): greedy wrt Q (deterministic)\")\n",
    "    logger.info(\"Importance Sampling: Weighted IS (lower variance than ordinary IS)\")\n",
    "    logger.info(f\"Parameters: gamma={gamma}, episodes={episodes}, max_steps={max_steps}, seed={seed}\")\n",
    "    logger.info(\"------------------------------------------------------\")\n",
    "\n",
    "    # Run Monte Carlo (WIS)\n",
    "    V_mc, pi_mc, t_mc = off_policy_mc_weighted_is(\n",
    "        gamma=gamma,\n",
    "        num_episodes=episodes,\n",
    "        max_steps=max_steps,\n",
    "        seed=seed,\n",
    "        log_every=log_every,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    logger.info(\"----- Final Results (MC Estimate) -----\")\n",
    "    logger.info(f\"Time={t_mc:.6f} sec\")\n",
    "    logger.info(\"V_MC (rounded to 3):\\n\" + str(np.round(V_to_grid(V_mc), 3)))\n",
    "    logger.info(\"pi_MC:\\n\" + str(policy_to_grid(pi_mc)))\n",
    "\n",
    "    # Compare MC estimate to Value Iteration optimum\n",
    "    t0_vi = time.time()\n",
    "    V_vi = value_iteration_sync(gamma=gamma, theta=1e-8)\n",
    "    t_vi = time.time() - t0_vi\n",
    "\n",
    "    grid_mc = V_to_grid(V_mc)\n",
    "    grid_vi = V_to_grid(V_vi)\n",
    "\n",
    "    mae = float(np.mean(np.abs(grid_mc - grid_vi)))\n",
    "    max_err = float(np.max(np.abs(grid_mc - grid_vi)))\n",
    "\n",
    "    logger.info(\"----- Comparison: MC vs Value Iteration -----\")\n",
    "    logger.info(f\"VI time={t_vi:.6f} sec\")\n",
    "    logger.info(f\"MAE(|V_MC - V*|) = {mae:.6f}\")\n",
    "    logger.info(f\"Max(|V_MC - V*|) = {max_err:.6f}\")\n",
    "    logger.info(\"======================================================\")\n",
    "\n",
    "    # Console output\n",
    "    print(\"Problem 4 complete. Log written to: logs/problem4.log\")\n",
    "    print(\"\\nV_MC (γ=0.95):\\n\", np.round(grid_mc, 3))\n",
    "    print(\"\\nπ_MC:\\n\", policy_to_grid(pi_mc))\n",
    "    print(\"\\nComparison vs VI (same γ):\")\n",
    "    print(\"  VI time =\", f\"{t_vi:.6f}s\")\n",
    "    print(\"  MAE(|V_MC - V*|) =\", f\"{mae:.6f}\")\n",
    "    print(\"  Max(|V_MC - V*|) =\", f\"{max_err:.6f}\")\n",
    "\n",
    "main() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d36094",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "Using off-policy Monte Carlo with Weighted Importance Sampling and a discount factor \\( gamma = 0.95 \\), the estimated value function \\( V_{\\{MC}} \\) and greedy policy \\( pi_{\\{MC}} \\) were obtained, with the goal state value close to 10 and the grey states showing lower values due to the −5 penalty. The learned policy mainly selects **Right (R)** and **Down (D)** actions, similar to the optimal policy from Value Iteration. Comparing \\( V_{\\{MC}} \\) with the optimal value function \\( V^* \\) gives a mean absolute error of about 0.045 and a maximum error of about 0.269, indicating that Monte Carlo provides a close but approximate solution due to sampling noise and a finite number of episodes. In terms of performance, Monte Carlo methods require many episodes and their cost grows with episode length, whereas Value Iteration is a model-based method with per-iteration complexity \\( O(|S||A|) \\) that converges in a small number of sweeps for this problem, highlighting the trade-off between model-free sampling methods and dynamic programming approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
